{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBIeUXvwvPCL4ZYHeThdjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safaet/mufti-llm/blob/main/MiniLLMWithDatabaseMemory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7-J4QudNcab",
        "outputId": "8a40987d-2fd9-4057-92af-9661656eab21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "\n",
            "Retrieved top context:\n",
            " id=3 sim=0.8934 text=Synapses are connections between neurons where neurotransmitters carry signals.\n",
            " id=1 sim=0.8921 text=The human brain works through electrical and chemical signals.\n",
            "\n",
            "Prompt to generator:\n",
            " Synapses are connections between neurons where neurotransmitters carry signals. The human brain works through electrical and chemical signals. how do neurons communicate\n",
            "\n",
            "Generated token ids (last 12): [1697, 789, 1689, 74, 1880, 1139, 1052, 804, 192, 1112, 924, 1658]\n",
            "\n",
            "Stored generated response into DB.\n",
            "\n",
            "âœ… Done. You can now modify the bottom part for interactive chat!\n"
          ]
        }
      ],
      "source": [
        "# =====================================================\n",
        "# ðŸ§  Mini LLM from scratch (step-by-step, no class)\n",
        "# =====================================================\n",
        "\n",
        "import sqlite3, io, numpy as np, torch, torch.nn.functional as F, os, random\n",
        "\n",
        "# -------------------------------\n",
        "# 0ï¸âƒ£ Setup\n",
        "# -------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -------------------------------\n",
        "# 1ï¸âƒ£ Model configuration\n",
        "# -------------------------------\n",
        "vocab_size = 2048\n",
        "d_model = 32\n",
        "n_heads = 4\n",
        "ff_hidden = 64\n",
        "max_seq_len = 24\n",
        "pad_token_id, bos_token_id, eos_token_id = 0, 1, 2\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# -------------------------------\n",
        "# 2ï¸âƒ£ Tokenization (hash-based)\n",
        "# -------------------------------\n",
        "def tokenize(text):\n",
        "    tokens = text.lower().split()\n",
        "    ids = []\n",
        "    for t in tokens[:max_seq_len]:\n",
        "        h = abs(hash(t)) % (vocab_size - 3) + 3\n",
        "        ids.append(h)\n",
        "    return ids\n",
        "\n",
        "# -------------------------------\n",
        "# 3ï¸âƒ£ Embedding initialization\n",
        "# -------------------------------\n",
        "embedding = torch.nn.Embedding(vocab_size, d_model).to(device)\n",
        "pos_embedding = torch.nn.Embedding(max_seq_len, d_model).to(device)\n",
        "\n",
        "# -------------------------------\n",
        "# 4ï¸âƒ£ Transformer weights (manual)\n",
        "# -------------------------------\n",
        "W_q = torch.randn(d_model, d_model, device=device) / (d_model ** 0.5)\n",
        "W_k = torch.randn(d_model, d_model, device=device) / (d_model ** 0.5)\n",
        "W_v = torch.randn(d_model, d_model, device=device) / (d_model ** 0.5)\n",
        "W_o = torch.randn(d_model, d_model, device=device) / (d_model ** 0.5)\n",
        "W_ff1 = torch.randn(d_model, ff_hidden, device=device) * (2.0 / (d_model ** 0.5))\n",
        "b_ff1 = torch.zeros(ff_hidden, device=device)\n",
        "W_ff2 = torch.randn(ff_hidden, d_model, device=device) * (2.0 / (ff_hidden ** 0.5))\n",
        "b_ff2 = torch.zeros(d_model, device=device)\n",
        "ln1 = torch.nn.LayerNorm(d_model).to(device)\n",
        "ln2 = torch.nn.LayerNorm(d_model).to(device)\n",
        "W_out = torch.randn(d_model, vocab_size, device=device) / (d_model ** 0.5)\n",
        "b_out = torch.zeros(vocab_size, device=device)\n",
        "\n",
        "for t in [embedding.weight, pos_embedding.weight, W_q, W_k, W_v, W_o,\n",
        "          W_ff1, b_ff1, W_ff2, b_ff2, W_out, b_out]:\n",
        "    if isinstance(t, torch.Tensor):\n",
        "        t.requires_grad = False\n",
        "\n",
        "# -------------------------------\n",
        "# 5ï¸âƒ£ Transformer block\n",
        "# -------------------------------\n",
        "def single_transformer_block(x):\n",
        "    batch, seq_len, _ = x.shape\n",
        "    Q = x @ W_q\n",
        "    K = x @ W_k\n",
        "    V = x @ W_v\n",
        "    head_dim = d_model // n_heads\n",
        "\n",
        "    def split_heads(t):\n",
        "        t = t.view(batch, seq_len, n_heads, head_dim)\n",
        "        return t.permute(0, 2, 1, 3)\n",
        "\n",
        "    Qh = split_heads(Q)\n",
        "    Kh = split_heads(K)\n",
        "    Vh = split_heads(V)\n",
        "\n",
        "    attn_scores = torch.matmul(Qh, Kh.transpose(-2, -1)) / (head_dim ** 0.5)\n",
        "    mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).unsqueeze(0).unsqueeze(0)\n",
        "    attn_scores = attn_scores.masked_fill(mask == 0, float('-1e9'))\n",
        "    attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "    attn_out = torch.matmul(attn_weights, Vh)\n",
        "    attn_out = attn_out.permute(0, 2, 1, 3).contiguous().view(batch, seq_len, d_model)\n",
        "    attn_out = attn_out @ W_o\n",
        "\n",
        "    x2 = ln1(x + attn_out)\n",
        "    ff = F.relu(x2 @ W_ff1 + b_ff1) @ W_ff2 + b_ff2\n",
        "    x3 = ln2(x2 + ff)\n",
        "    return x3\n",
        "\n",
        "# -------------------------------\n",
        "# 6ï¸âƒ£ Forward pass for logits\n",
        "# -------------------------------\n",
        "def forward_logits(token_ids):\n",
        "    seq_len = len(token_ids)\n",
        "    toks = torch.tensor(token_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "    pos_ids = torch.tensor(list(range(seq_len)), dtype=torch.long, device=device).unsqueeze(0)\n",
        "    x = embedding(toks) + pos_embedding(pos_ids)\n",
        "    x = single_transformer_block(x)\n",
        "    logits = x @ W_out + b_out\n",
        "    return logits\n",
        "\n",
        "# -------------------------------\n",
        "# 7ï¸âƒ£ Text generation\n",
        "# -------------------------------\n",
        "def generate(prompt_text, max_new_tokens=8, temperature=1.0, top_k=30):\n",
        "    token_ids = tokenize(prompt_text)\n",
        "    token_ids = token_ids[-(max_seq_len-1):]\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = forward_logits(token_ids)\n",
        "        last_logits = logits[0, -1, :] / (temperature if temperature>0 else 1.0)\n",
        "        K = min(top_k, last_logits.shape[0])\n",
        "        values, indices = torch.topk(last_logits, k=K)\n",
        "        probs = F.softmax(values, dim=-1)\n",
        "        next_id = indices[torch.multinomial(probs, num_samples=1).item()].item()\n",
        "        token_ids.append(next_id)\n",
        "        if next_id == eos_token_id:\n",
        "            break\n",
        "        if len(token_ids) > max_seq_len:\n",
        "            token_ids = token_ids[-max_seq_len:]\n",
        "    return token_ids\n",
        "\n",
        "# =====================================================\n",
        "# ðŸ’¾ Simple Database Memory (SQLite)\n",
        "# =====================================================\n",
        "db_path = \"mini_llm_store.sqlite\"\n",
        "if os.path.exists(db_path):\n",
        "    os.remove(db_path)\n",
        "conn = sqlite3.connect(db_path)\n",
        "c = conn.cursor()\n",
        "c.execute(\"CREATE TABLE kv (id INTEGER PRIMARY KEY AUTOINCREMENT, text TEXT, emb BLOB)\")\n",
        "conn.commit()\n",
        "\n",
        "# -------------------------------\n",
        "# Helper: vector save/load\n",
        "# -------------------------------\n",
        "def numpy_to_blob(x):\n",
        "    memfile = io.BytesIO()\n",
        "    np.save(memfile, x)\n",
        "    memfile.seek(0)\n",
        "    return memfile.read()\n",
        "\n",
        "def blob_to_numpy(b):\n",
        "    memfile = io.BytesIO(b)\n",
        "    memfile.seek(0)\n",
        "    return np.load(memfile)\n",
        "\n",
        "# -------------------------------\n",
        "# Compute sentence embedding\n",
        "# -------------------------------\n",
        "def sentence_embedding(text):\n",
        "    ids = tokenize(text)\n",
        "    if len(ids) == 0:\n",
        "        return np.zeros(d_model, dtype=np.float32)\n",
        "    toks = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "    pos_ids = torch.tensor(list(range(len(ids))), dtype=torch.long, device=device).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        x = embedding(toks) + pos_embedding(pos_ids)\n",
        "        x = single_transformer_block(x)\n",
        "        vec = x.mean(dim=1).cpu().numpy().astype(np.float32)\n",
        "    return np.squeeze(vec)\n",
        "\n",
        "def store_sentence(text):\n",
        "    emb = sentence_embedding(text)\n",
        "    b = numpy_to_blob(emb)\n",
        "    c.execute(\"INSERT INTO kv (text, emb) VALUES (?, ?)\", (text, b))\n",
        "    conn.commit()\n",
        "    return c.lastrowid\n",
        "\n",
        "def retrieve_similar(query, top_k=3):\n",
        "    q_emb = sentence_embedding(query)\n",
        "    c.execute(\"SELECT id, text, emb FROM kv\")\n",
        "    rows = c.fetchall()\n",
        "    results = []\n",
        "    for rid, txt, b in rows:\n",
        "        emb = blob_to_numpy(b)\n",
        "        denom = (np.linalg.norm(q_emb) * np.linalg.norm(emb))\n",
        "        sim = float(np.dot(q_emb, emb) / denom) if denom > 0 else 0.0\n",
        "        results.append((rid, txt, sim))\n",
        "    results.sort(key=lambda x: x[2], reverse=True)\n",
        "    return results[:top_k]\n",
        "\n",
        "# =====================================================\n",
        "# ðŸ§© Demo: store, retrieve, generate\n",
        "# =====================================================\n",
        "sample_sentences = [\n",
        "    \"The human brain works through electrical and chemical signals.\",\n",
        "    \"Neurons communicate by sending electrical impulses called action potentials.\",\n",
        "    \"Synapses are connections between neurons where neurotransmitters carry signals.\",\n",
        "    \"The brain consumes a lot of energy and oxygen to function properly.\",\n",
        "    \"Learning changes the strength of synapses in a process called synaptic plasticity.\"\n",
        "]\n",
        "for s in sample_sentences:\n",
        "    store_sentence(s)\n",
        "\n",
        "demo_query = \"how do neurons communicate\"\n",
        "retrieved = retrieve_similar(demo_query, top_k=2)\n",
        "print(\"\\nRetrieved top context:\")\n",
        "for rid, txt, sim in retrieved:\n",
        "    print(f\" id={rid} sim={sim:.4f} text={txt}\")\n",
        "\n",
        "context = \" \".join([txt for (_, txt, _) in retrieved])\n",
        "prompt = context + \" \" + demo_query\n",
        "\n",
        "print(\"\\nPrompt to generator:\\n\", prompt)\n",
        "generated_ids = generate(prompt, max_new_tokens=6, temperature=0.8, top_k=20)\n",
        "print(\"\\nGenerated token ids (last 12):\", generated_ids[-12:])\n",
        "store_sentence(\"AUTO: \" + \" \".join(map(str, generated_ids[-12:])))\n",
        "print(\"\\nStored generated response into DB.\")\n",
        "print(\"\\nâœ… Done. You can now modify the bottom part for interactive chat!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_query = input(\"You: \")\n",
        "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "    retrieved = retrieve_similar(user_query)\n",
        "    # Modify the line below to unpack only the text\n",
        "    context = \" \".join([txt for txt in retrieved])\n",
        "    prompt = context + \" \" + user_query\n",
        "    generated = generate(prompt, max_new_tokens=8)\n",
        "    print(\"LLM:\", generated[-8:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGgD0iy1OuXF",
        "outputId": "35ac0dda-4514-424c-ea25-5e2a98771113"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: what is your name\n",
            "LLM: ame? the\n",
            "You: safaet\n",
            "LLM: ure. the\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# MINI LLM WITH READABLE OUTPUT\n",
        "# =============================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "\n",
        "# ---------- DEVICE ----------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ---------- PARAMETERS ----------\n",
        "vocab = {}\n",
        "reverse_vocab = {}\n",
        "vocab_size = 1  # will grow dynamically\n",
        "max_seq_len = 50\n",
        "d_model = 64\n",
        "num_heads = 2\n",
        "d_ff = 128\n",
        "\n",
        "# ---------- TOKENIZER ----------\n",
        "def tokenize(text):\n",
        "    global vocab, reverse_vocab, vocab_size\n",
        "    tokens = text.lower().split()\n",
        "    ids = []\n",
        "    for t in tokens:\n",
        "        if t not in vocab:\n",
        "            vocab[t] = vocab_size\n",
        "            reverse_vocab[vocab_size] = t\n",
        "            vocab_size += 1\n",
        "        ids.append(vocab[t])\n",
        "    return torch.tensor(ids, dtype=torch.long, device=device)\n",
        "\n",
        "def detokenize(ids):\n",
        "    words = [reverse_vocab.get(i, \"<unk>\") for i in ids]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# ---------- EMBEDDINGS ----------\n",
        "# Increased embedding size to account for potential vocabulary growth\n",
        "word_embeddings = nn.Embedding(10000, d_model).to(device)\n",
        "pos_embeddings = nn.Embedding(max_seq_len, d_model).to(device)\n",
        "\n",
        "# ---------- TRANSFORMER COMPONENTS ----------\n",
        "def attention(q, k, v, mask=None):\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    weights = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(weights, v)\n",
        "\n",
        "def multi_head_attention(x):\n",
        "    B, T, C = x.shape\n",
        "    head_dim = C // num_heads\n",
        "    qkv = nn.Linear(C, C * 3, device=device)(x)\n",
        "    q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "    # Reshape and transpose for multi-head attention\n",
        "    q = q.view(B, T, num_heads, head_dim).transpose(1, 2)\n",
        "    k = k.view(B, T, num_heads, head_dim).transpose(1, 2)\n",
        "    v = v.view(B, T, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "    attn_out = attention(q, k, v)\n",
        "\n",
        "    # Concatenate heads and apply output linear layer\n",
        "    attn_out = attn_out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "    return nn.Linear(C, C, device=device)(attn_out)\n",
        "\n",
        "\n",
        "def feed_forward(x):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(d_model, d_ff, device=device),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(d_ff, d_model, device=device)\n",
        "    )(x)\n",
        "\n",
        "# ---------- TRANSFORMER BLOCK ----------\n",
        "def transformer_block(x):\n",
        "    attn_out = multi_head_attention(x)\n",
        "    x = x + attn_out\n",
        "    x = nn.LayerNorm(d_model, device=device)(x)\n",
        "    ff_out = feed_forward(x)\n",
        "    x = x + ff_out\n",
        "    x = nn.LayerNorm(d_model, device=device)(x)\n",
        "    return x\n",
        "\n",
        "# ---------- MINI LLM FORWARD ----------\n",
        "def mini_llm_forward(input_ids):\n",
        "    positions = torch.arange(0, input_ids.size(1), device=device) # Changed to use size(1) for batch processing\n",
        "    x = word_embeddings(input_ids) + pos_embeddings(positions)\n",
        "    x = transformer_block(x)\n",
        "    logits = nn.Linear(d_model, vocab_size, device=device)(x) # Changed to use dynamic vocab_size\n",
        "    return logits\n",
        "\n",
        "# ---------- GENERATION ----------\n",
        "def generate(prompt, max_new_tokens=15):\n",
        "    ids = tokenize(prompt).unsqueeze(0) # Add batch dimension for forward pass\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Ensure sequence length does not exceed max_seq_len\n",
        "        if ids.size(1) >= max_seq_len:\n",
        "            ids = ids[:, -max_seq_len:]\n",
        "        logits = mini_llm_forward(ids)\n",
        "        next_token_logits = logits[0, -1, :]\n",
        "        probs = F.softmax(next_token_logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1).unsqueeze(0) # Add batch dimension\n",
        "        ids = torch.cat([ids, next_id], dim=1) # Concatenate along the sequence dimension\n",
        "        if next_id.item() == vocab.get(\"<eos>\", -1):\n",
        "            break\n",
        "    return detokenize(ids.squeeze(0).tolist()) # Remove batch dimension for detokenization\n",
        "\n",
        "# ---------- VECTOR STORE (SQLite) ----------\n",
        "conn = sqlite3.connect(\"memory.db\")\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"CREATE TABLE IF NOT EXISTS memory (text TEXT, vector BLOB)\")\n",
        "conn.commit()\n",
        "\n",
        "def store_text(text):\n",
        "    # Ensure text is not empty before processing\n",
        "    if not text.strip():\n",
        "        return\n",
        "    ids = tokenize(text).unsqueeze(0) # Add batch dimension\n",
        "    if ids.size(1) == 0: # Handle empty token list\n",
        "        return\n",
        "    with torch.no_grad(): # Use no_grad for inference\n",
        "        vec = word_embeddings(ids).mean(dim=1).detach().cpu().numpy() # Mean along sequence dimension\n",
        "    cur.execute(\"INSERT INTO memory VALUES (?, ?)\", (text, vec.tobytes()))\n",
        "    conn.commit()\n",
        "\n",
        "def retrieve_similar(query, top_k=2):\n",
        "    # Ensure query is not empty before processing\n",
        "    if not query.strip():\n",
        "        return []\n",
        "    q_ids = tokenize(query).unsqueeze(0) # Add batch dimension\n",
        "    if q_ids.size(1) == 0: # Handle empty token list\n",
        "        return []\n",
        "    with torch.no_grad(): # Use no_grad for inference\n",
        "        q_vec = word_embeddings(q_ids).mean(dim=1).detach().cpu().numpy() # Mean along sequence dimension\n",
        "    cur.execute(\"SELECT text, vector FROM memory\")\n",
        "    results = []\n",
        "    for text, blob in cur.fetchall():\n",
        "        vec = np.frombuffer(blob, dtype=np.float32).reshape(q_vec.shape) # Reshape loaded vector\n",
        "        denom = (np.linalg.norm(q_vec) * np.linalg.norm(vec))\n",
        "        sim = np.dot(q_vec.flatten(), vec.flatten()) / (denom + 1e-8) # Flatten for dot product\n",
        "        results.append((sim, text))\n",
        "    results.sort(reverse=True)\n",
        "    return [t for _, t in results[:top_k]]\n",
        "\n",
        "# ---------- DEMO ----------\n",
        "store_text(\"The sun rises in the east.\")\n",
        "store_text(\"Artificial intelligence is the future.\")\n",
        "store_text(\"Cats are cute and love to sleep.\")\n",
        "\n",
        "query = \"Tell me about the sun\"\n",
        "context = retrieve_similar(query)\n",
        "prompt = \" \".join(context) + \" \" + query\n",
        "print(\"Prompt with context:\\n\", prompt)\n",
        "print(\"\\nGenerated:\\n\", generate(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytdzVlQUQKLD",
        "outputId": "0252b299-192f-4e07-b580-95ad35c60ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Prompt with context:\n",
            " The sun rises in the east. Artificial intelligence is the future. Tell me about the sun\n",
            "\n",
            "Generated:\n",
            " the sun rises in the east. artificial intelligence is the future. tell me about the sun and artificial future. in rises cats rises tell cats east. about are sun love and\n"
          ]
        }
      ]
    }
  ]
}