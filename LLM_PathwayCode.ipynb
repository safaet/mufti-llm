{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMg+i4SHIEzf64cXa0vpE3k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safaet/mufti-llm/blob/main/LLM_PathwayCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text -> Tokenize -> Embed -> Transformer -> Predict next token**"
      ],
      "metadata": {
        "id": "NmeCvh4T81i8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx9gJouB6mqL",
        "outputId": "27d65a7b-104d-41a3-c708-17e324ad1e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: loss = 2.5907\n",
            "Step 50: loss = 0.4702\n",
            "Step 100: loss = 0.4357\n",
            "Step 150: loss = 0.0509\n",
            "Step 200: loss = 0.2567\n",
            "Step 250: loss = 0.0602\n",
            "Step 300: loss = 0.3255\n",
            "Step 350: loss = 0.3148\n",
            "Step 400: loss = 0.2262\n",
            "Step 450: loss = 0.8030\n",
            "\n",
            "Generated Text:\n",
            "hanglattttograd Chatt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ------------------------------\n",
        "# 1. TEXT & TOKENIZATION\n",
        "# ------------------------------\n",
        "\n",
        "text = \"hello world Chattogram bangladesh\"\n",
        "chars = sorted(set(text))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Mappings: char â†” index\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(ids): return ''.join([itos[i] for i in ids])\n",
        "\n",
        "# Example data\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# ------------------------------\n",
        "# 2. MODEL DEFINITION\n",
        "# ------------------------------\n",
        "\n",
        "class MiniTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim, embed_dim)\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attn(x, x, x, need_weights=False)\n",
        "        x = self.ln1(x + attn_output)\n",
        "        x = self.ln2(x + self.ff(x))\n",
        "        return x\n",
        "\n",
        "class MiniLLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, block_size=8, num_heads=2):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
        "\n",
        "        self.transformer = MiniTransformerBlock(embed_dim, num_heads)\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        token_embed = self.token_embedding(idx)\n",
        "        pos = torch.arange(T, device=idx.device)\n",
        "        pos_embed = self.position_embedding(pos)\n",
        "        x = token_embed + pos_embed\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "# ------------------------------\n",
        "# 3. TRAINING SETUP\n",
        "# ------------------------------\n",
        "\n",
        "model = MiniLLM(vocab_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def get_batch(seq_len=8):\n",
        "    start = torch.randint(0, len(data) - seq_len - 1, (1,)).item()\n",
        "    x = data[start:start+seq_len]\n",
        "    y = data[start+1:start+seq_len+1]\n",
        "    return x.unsqueeze(0), y.unsqueeze(0)  # (B=1, T)\n",
        "\n",
        "# ------------------------------\n",
        "# 4. TRAINING LOOP\n",
        "# ------------------------------\n",
        "\n",
        "for step in range(500):\n",
        "    x, y = get_batch()\n",
        "    logits = model(x)\n",
        "    loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(f\"Step {step}: loss = {loss.item():.4f}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5. TEXT GENERATION\n",
        "# ------------------------------\n",
        "\n",
        "def generate(model, start_text='h', max_new_tokens=20):\n",
        "    model.eval()\n",
        "    idx = torch.tensor([encode(start_text)], dtype=torch.long)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_crop = idx[:, -model.block_size:]\n",
        "        logits = model(idx_crop)\n",
        "        logits = logits[:, -1, :]  # last time step\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(generate(model, start_text=\"h\"))\n"
      ]
    }
  ]
}